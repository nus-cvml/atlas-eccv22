<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="main.css" type="text/css" />
<link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
<!--- <title></title> --->
<title>ATLAS Tutorial ECCV22</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="main-container">
<div id="header-container">
<div id="header">
<div id="header-icon-text-container">
<!--
<div id="header-icon-container" >
<a href="index.html"><img src="./images/icon.png" alt="" style="width: 100%; height: 100%; position: center; padding:0px; margin: 0px;"></a>
</div>
-->
<div id="header-text-container">
<a href="index.html">ATLAS Tutorial</a>
</div>
</div>
<div id="main">
<button class="openbtn" onclick="openNav()">☰</button>
</div>
</div>
</div>
<div id="layout-content">
<div id="text-img-container"><div id="img-container">
<a href="https://atlas-eccv22.github.io"><img src="./logos/atlas.png" alt="atlas" width="100%" /></a></div>
<div id="text-container"></div></div>
<p>
<div id="beamer">
<beam> 
<b>A</b>c<b>T</b>ion <b>L</b>ocalization <b>A</b>nd <b>S</b>egmentation in Untrimmed Videos
</beam></br>
<beams>
in conjunction with ECCV 2022, TEL AVIV</br>
Monday Morning, October 24, 2022 
</beams>
</div>
</p>
<h1>Overview </h1>
<p>The majority of research in action understanding focuses on designing methods to encode a few seconds of short, trimmed clips and classify these with single action labels. 
Such methods, however, are rarely applicable for temporally localizing and/or classifying actions from longer, untrimmed streams of video. 
In this tutorial, we would like to focus on research on understanding actions in untrimmed, long videos up to tens of minutes.<br /></p>
<p>Compared to action recognition from trimmed video clips, untrimmed long video understanding tasks pose more challenges due to the long span of videos and complex temporal relations between occurring actions. 
Such challenges include: <i>“What are the actions and when do these actions happen in the untrimmed long video sequences?”</i> 
Our main focus for this tutorial is two tasks that aim to find human actions in videos, <i>i.e.</i>,  Temporal Action Localization/Detection (TAL/D) and Temporal Action Segmentation (TAS).</p>
<h1>Speakers</h1>
<p>TBD</p>
<h1>Schedule</h1>
<p>TBD</p>
<h1>Organizers</h1>
<p>
<div id="member-container">
<div id="member">
<img src="./profiles/angela.png">
<p>
<b>Angela Yao</b></br>
Assistant Professor, SoC NUS
</p>
</div>
<div id="member">
<img src="./profiles/junsong.png">
<p>
<b>Junsong Yuan</b></br>
Professor, CSE SUNY Buffalo
</p>
</div> 
<div id="member">
<img src="./profiles/fadime.png">
<p>
<b>Fadime Sener</b></br>
Research Scientist, Reality Labs, Meta
</p>
</div>
<div id="member">
<img src="./profiles/guodong.png">
<p>
<b>Guodong Ding</b></br>
Research Fellow, SoC NUS
</p>
</div>
</div>
</p>
<div id="footer">
<div id="footer-text">
Last edited  on July 7<sup>th</sup> 2022  08:00PM (Time Zone: +08). </br>
</div>
</div>
</div>
</body>
</html>
