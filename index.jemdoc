
~~~
{}{img_left}{./logos/atlas.png}{atlas}{100%}{}{https://atlas-eccv22.github.io}
~~~

{{
<div id="beamer">
<beam> 
<b>A</b>c<b>T</b>ion <b>L</b>ocalization <b>A</b>nd <b>S</b>egmentation in Untrimmed Videos
</beam></br>
<beams>
in conjunction with ECCV 2022, TEL AVIV</br>
Monday Morning, October 24, 2022 
</beams>
</div>
}}

= Overview 
The majority of research in action understanding focuses on designing methods to encode a few seconds of short, trimmed clips and classify these with single action labels. 
Such methods, however, are rarely applicable for temporally localizing and\/or classifying actions from longer, untrimmed streams of video. 
In this tutorial, we would like to focus on research on understanding actions in untrimmed, long videos up to tens of minutes.\n
    
Compared to action recognition from trimmed video clips, untrimmed long video understanding tasks pose more challenges due to the long span of videos and complex temporal relations between occurring actions. 
Such challenges include: /“What are the actions and when do these actions happen in the untrimmed long video sequences?”/ 
Our main focus for this tutorial is two tasks that aim to find human actions in videos, /i.e./,  Temporal Action Localization\/Detection (TAL\/D) and Temporal Action Segmentation (TAS).

= Speakers
{{
<div id="member-container">
    <div id="member">
        <img src="./profiles/angela.png">
        <p>
            <b><a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a></b></br>
            Assistant Professor, SoC NUS
        </p>
    </div>
    <div id="member">
        <img src="./profiles/junsong.png">
        <p>
            <b><a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a></b></br>
            Professor, CSE SUNY Buffalo
        </p>
    </div> 
    <div id="member">
        <img src="./profiles/ehsan.png">
        <p>
            <b><a href="https://www.ccs.neu.edu/home/eelhami/">Ehsan Elhamifar</a></b></br>
            Associate Professor, Northeastern Univeristy
        </p>
    </div>
        <div id="member">
        <img src="./profiles/hilde.png">
        <p>
            <b><a href="https://hildekuehne.github.io/">Hilde Kuehne</a></b></br>
            Assistant Professor, Goethe Univeristy
        </p>
    </div>
        <div id="member">
        <img src="./profiles/yougesh.png">
        <p>
            <b><a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a></b></br>
            Assistant Professor, University of Central Florida
        </p>
    </div>
</div>
}}

= Schedule
You may join us via Zoom 
- Opening Remarks (8:10 - 8:20am, Organizers) 
- Title (9:20 - 9:00am, Yao) \[[video]\] #\[[ Resources]\]|\[[Slides]\]|\[[YouTube]\]|\[[Survey]\]
- Title (9:00 - 9:40am, Yuan) \[[video]\] \[[slide]\]
- Coffee Break (9:40 - 9:50am) 
- Title (9:50 - 10:30, Kuehne) \[[video]\] \[[slide]\]
- Title (10:30 - 11:10am, Elhamifar) \[[video]\] \[[slide]\]
- Title (11:10 - 11:50am, Rawat) \[[video]\] \[[slide]\]

= Resources
- Awesome-temporal-action-segmentation \[[https://github.com/atlas-eccv22/awesome-temporal-action-segmentation Github]\]
- Temporal Action Segmentation: An analysis of Modern Tehcnique \[[PDF]\]

= Organizers
{{
<div id="member-container">
    <div id="member">
        <img src="./profiles/angela.png">
        <p>
            <b>Angela Yao</b></br>
            Assistant Professor, SoC NUS
        </p>
    </div>
    <div id="member">
        <img src="./profiles/junsong.png">
        <p>
            <b>Junsong Yuan</b></br>
            Professor, CSE SUNY Buffalo
        </p>
    </div> 
    <div id="member">
        <img src="./profiles/fadime.png">
        <p>
            <b>Fadime Sener</b></br>
            Research Scientist, Reality Labs, Meta
        </p>
    </div>
    <div id="member">
        <img src="./profiles/guodong.png">
        <p>
            <b>Guodong Ding</b></br>
            Research Fellow, SoC NUS
        </p>
    </div>
</div>
}}
